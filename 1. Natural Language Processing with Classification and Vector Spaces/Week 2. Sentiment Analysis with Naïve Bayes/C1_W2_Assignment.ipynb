{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayes\n",
    "Welcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n",
    "\n",
    "* Train a naive bayes model on a sentiment analysis task\n",
    "* Test using your model\n",
    "* Compute ratios of positive words to negative words\n",
    "* Do some error analysis\n",
    "* Predict on your own tweet\n",
    "\n",
    "You may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence.\n",
    "* In this week's lectures and assignments we used the ratio of probabilities between positive and negative sentiment.\n",
    "* This approach gives us simpler formulas for these 2-way classification tasks.\n",
    "\n",
    "Load the cell below to import some packages.\n",
    "You  may want to browse the documentation of unfamiliar libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import process_tweet, lookup\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "import w2_unittest\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook in your local computer,\n",
    "don't forget to download the tweeter samples and stopwords from nltk.\n",
    "\n",
    "```\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twetter_samples')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Process the Data\n",
    "\n",
    "For any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n",
    "- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n",
    "- We'll also remove stock market tickers, retweet symbols, hyperlinks, and hashtags because they can not tell you a lot of information on the sentiment.\n",
    "- You also want to remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n",
    "- Finally you want to use stemming to only keep track of one variation of each word. In other words, we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n",
    "\n",
    "We have given you the function `process_tweet` that does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 Implementing your helper functions\n",
    "\n",
    "To help you train your naive bayes model, you will need to compute a dictionary where the keys are a tuple (word, label) and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative.\n",
    "\n",
    "You will also implement a lookup helper function that takes in the `freqs` dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n",
    "\n",
    "For example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
    "\n",
    "{\n",
    "    (\"rather\", 1): 2,\n",
    "    (\"happi\", 1) : 1, \n",
    "    (\"excit\", 1) : 1\n",
    "}\n",
    "\n",
    "- Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
    "- Notice how the words \"i\" and \"am\" are not saved, since it was removed by process_tweet because it is a stopword.\n",
    "- Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n",
    "\n",
    "#### Instructions\n",
    "Create a function `count_tweets` that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n",
    "- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n",
    "- The value the number of times this word appears in the given collection of tweets (an integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Please use the `process_tweet` function that was imported above, and then store the words in their respective dictionaries and sets.</li>\n",
    "    <li>You may find it useful to use the `zip` function to match each element in `tweets` with each element in `ys`.</li>\n",
    "    <li>Remember to check if the key in the dictionary exists before adding that key to the dictionary, or incrementing its value.</li>\n",
    "    <li>Assume that the `result` dictionary that is input will contain clean key-value pairs (you can assume that the values will be integers that can be incremented).  It is good practice to check the datatype before incrementing the value, but it's not required here.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 GRADED FUNCTION: count_tweets\n",
    "\n",
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word, y)\n",
    "            \n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your function\n",
    "\n",
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_count_tweets(count_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Train your model using Naive Bayes\n",
    "\n",
    "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
    "\n",
    "#### So how do you train a Naive Bayes classifier?\n",
    "- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n",
    "- You will create a probability for each class.\n",
    "$P(D_{pos})$ is the probability that the document is positive.\n",
    "$P(D_{neg})$ is the probability that the document is negative.\n",
    "Use the formulas as follows and store the values in a dictionary:\n",
    "\n",
    "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
    "\n",
    "Where $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior and Logprior\n",
    "\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "\n",
    "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
    "\n",
    "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
    "\n",
    "Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n",
    "\n",
    "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive and Negative Probability of a Word\n",
    "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
    "\n",
    "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
    "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
    "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
    "\n",
    "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "Notice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create `freqs` dictionary\n",
    "- Given your `count_tweets` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
    "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
    "- The value is the number of times it has appeared.\n",
    "\n",
    "We will use this dictionary in several parts of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
    "\n",
    "##### Calculate $V$\n",
    "- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n",
    "\n",
    "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
    "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
    "\n",
    "##### Calculate $N_{pos}$, and $N_{neg}$\n",
    "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
    "\n",
    "##### Calculate $D$, $D_{pos}$, $D_{neg}$\n",
    "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n",
    "- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n",
    "\n",
    "##### Calculate the logprior\n",
    "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
    "\n",
    "##### Calculate log likelihood\n",
    "- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
    "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n",
    "\n",
    "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 GRADED FUNCTION: train_naive_bayes\n",
    "\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos/D_neg)\n",
    "    \n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1)/(N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1)/(N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9165\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "0.0\n",
    "\n",
    "9165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong values for loglikelihood dictionary. Please check your implementation for the loglikelihood dictionary.\n",
      "Wrong values for loglikelihood dictionary. Please check your implementation for the loglikelihood dictionary.\n",
      "Wrong values for loglikelihood dictionary. Please check your implementation for the loglikelihood dictionary.\n",
      "\u001b[92m 12  Tests passed\n",
      "\u001b[91m 3  Tests failed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_train_naive_bayes(train_naive_bayes, freqs, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Test your naive bayes\n",
    "\n",
    "Now that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!\n",
    "\n",
    "#### Implement `naive_bayes_predict`\n",
    "**Instructions**:\n",
    "Implement the `naive_bayes_predict` function to make predictions on tweets.\n",
    "* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n",
    "* It returns the probability that the tweet belongs to the positive or negative class.\n",
    "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
    "* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n",
    "\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
    "\n",
    "#### Note\n",
    "Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative 1, and the logprior is 0.\n",
    "\n",
    "The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.  However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 GRADED FUNCTION: naive_bayes_predict\n",
    "\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 3.5099798589656745\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# Experiment with your own tweet.\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- The expected output is around 1.55\n",
    "- The sentiment is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_naive_bayes_predict(naive_bayes_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement test_naive_bayes\n",
    "**Instructions**:\n",
    "* Implement `test_naive_bayes` to check the accuracy of your predictions.\n",
    "* The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood\n",
    "* It returns the accuracy of your model.\n",
    "* First, use `naive_bayes_predict` function to make predictions for each tweet in text_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 GRADED FUNCTION: test_naive_bayes\n",
    "\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats - test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.7170\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Accuracy**:\n",
    "\n",
    "`Naive Bayes accuracy = 0.9955`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 4.09\n",
      "I am bad -> 0.64\n",
      "this movie should have been great. -> 6.02\n",
      "great -> 4.08\n",
      "great great -> 8.16\n",
      "great great great -> 12.24\n",
      "great great great great -> 16.33\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:    \n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- I am happy -> 2.14\n",
    "- I am bad -> -1.31\n",
    "- this movie should have been great. -> 2.12\n",
    "- great -> 2.13\n",
    "- great great -> 4.26\n",
    "- great great great -> 6.39\n",
    "- great great great great -> 8.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.932988404942234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.unittest_test_naive_bayes(test_naive_bayes, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Filter words by Ratio of positive to negative counts\n",
    "\n",
    "- Some words have more positive counts than others, and can be considered \"more positive\".  Likewise, some words can be considered more negative than others.\n",
    "- One way for us to define the level of positiveness or negativeness, without calculating the log likelihood, is to compare the positive to negative frequency of the word.\n",
    "    - Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words.\n",
    "- We can calculate the ratio of positive to negative frequencies of a word.\n",
    "- Once we're able to calculate these ratios, we can also filter a subset of words that have a minimum ratio of positivity / negativity or higher.\n",
    "- Similarly, we can also filter a subset of words that have a maximum ratio of positivity / negativity or lower (words that are at least as negative, or even more negative than a given threshold).\n",
    "\n",
    "#### Implement get_ratio\n",
    "- Given the freqs dictionary of words and a particular word, use `lookup(freqs,word,1)` to get the positive count of the word.\n",
    "- Similarly, use the `lookup` function to get the negative count of that word.\n",
    "- Calculate the ratio of positive divided by negative counts\n",
    "\n",
    "$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$\n",
    "\n",
    "Where pos_words and neg_words correspond to the frequency of the words in their respective classes. \n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Words</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        Positive word count\n",
    "        </td>\n",
    "         <td>\n",
    "        Negative Word Count\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        glad\n",
    "        </td>\n",
    "         <td>\n",
    "        41\n",
    "        </td>\n",
    "    <td>\n",
    "        2\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        arriv\n",
    "        </td>\n",
    "         <td>\n",
    "        57\n",
    "        </td>\n",
    "    <td>\n",
    "        4\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        :(\n",
    "        </td>\n",
    "         <td>\n",
    "        1\n",
    "        </td>\n",
    "    <td>\n",
    "        3663\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        :-(\n",
    "        </td>\n",
    "         <td>\n",
    "        0\n",
    "        </td>\n",
    "    <td>\n",
    "        378\n",
    "        </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 GRADED FUNCTION: get_ratio\n",
    "\n",
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    ### START CODE HERE ###\n",
    "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "    pos_neg_ratio['positive'] = lookup(freqs, word, 1)\n",
    "    \n",
    "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "    pos_neg_ratio['negative'] = lookup(freqs, word, 0)\n",
    "    \n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (\n",
    "        pos_neg_ratio['positive'] + 1) / (pos_neg_ratio['negative'] + 1)\n",
    "    ### END CODE HERE ###\n",
    "    return pos_neg_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 162, 'negative': 18, 'ratio': 8.578947368421053}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'happi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_get_ratio(get_ratio, freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement get_words_by_threshold(freqs,label,threshold)\n",
    "\n",
    "* If we set the label to 1, then we'll look for all words whose threshold of positive/negative is at least as high as that threshold, or higher.\n",
    "* If we set the label to 0, then we'll look for all words whose threshold of positive/negative is at most as low as the given threshold, or lower.\n",
    "* Use the `get_ratio` function to get a dictionary containing the positive count, negative count, and the ratio of positive to negative counts.\n",
    "* Append the `get_ratio` dictionary inside another dictinoary, where the key is the word, and the value is the dictionary `pos_neg_ratio` that is returned by the `get_ratio` function.\n",
    "An example key-value pair would have this structure:\n",
    "```\n",
    "{'happi':\n",
    "    {'positive': 10, 'negative': 20, 'ratio': 0.524}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C9 GRADED FUNCTION: get_words_by_threshold\n",
    "\n",
    "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary of words\n",
    "        label: 1 for positive, 0 for negative\n",
    "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    Output:\n",
    "        word_list: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "        example of a key value pair:\n",
    "        {'happi':\n",
    "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "        }\n",
    "    '''\n",
    "    word_list = {}\n",
    "    # CODE REVIEW COMMENT: This has been changed!! word_list was described as a dictionary, but defined (and operated on) as a list\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "\n",
    "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "        \n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "        \n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # otherwise, do not include this word in the list (do nothing)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':(': {'positive': 1, 'negative': 3675, 'ratio': 0.000544069640914037},\n",
       " ':-(': {'positive': 0, 'negative': 386, 'ratio': 0.002583979328165375},\n",
       " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
       " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
       " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '♛': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " '》': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " 'beli̇ev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'wi̇ll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'justi̇n': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｓｅｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｍｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function: find negative words at or below a threshold\n",
    "get_words_by_threshold(freqs, label=0, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
       " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
       " ':)': {'positive': 2960, 'negative': 2, 'ratio': 987.0},\n",
       " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':D': {'positive': 523, 'negative': 0, 'ratio': 524.0},\n",
       " ':p': {'positive': 104, 'negative': 0, 'ratio': 105.0},\n",
       " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':-)': {'positive': 552, 'negative': 0, 'ratio': 553.0},\n",
       " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
       " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
       " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
       " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
       " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
       " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
       " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " 'fantast': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
       " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
       " '←': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'aqui': {'positive': 9, 'negative': 0, 'ratio': 10.0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function; find positive words at or above a threshold\n",
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference between the positive and negative ratios. Emojis like :( and words like 'me' tend to have a negative connotation. Other words like glad, community, arrives, tend to be found in the positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_get_words_by_threshold(get_words_by_threshold, freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Error Analysis\n",
    "\n",
    "In this part you will see some tweets that your model missclassified. Why do you think the missclassifications happened? Were there any assumptions made by your naive bayes model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "0\t1.00\tb'soon tweet plant claw thigh traction zoom away :('\n",
      "0\t1.00\tb'use pri pv ... wish could reliv day becom nyc pv buy way commun nyc usa klm :('\n",
      "0\t1.00\tb'realli hot :-('\n",
      "0\t1.00\tb\"go stop breakfast earli might want remov 11am websit even mcd' pull trick :(\"\n",
      "0\t1.00\tb'live fam bam cough :('\n",
      "0\t1.00\tb'absolut gut jame bay ticket sold manchest :('\n",
      "0\t1.00\tb'sore alic :('\n",
      "0\t1.00\tb\"i'm readi work yet :(\"\n",
      "0\t1.00\tb'go spend night pragu leav tomorrow :('\n",
      "0\t1.00\tb'one friend follow littl heart attack im sorri your block :( sadi'\n",
      "0\t1.00\tb'consid lucki favourit charact even make season final 100 :('\n",
      "0\t1.00\tb'boo alreadi turn ye 915 :('\n",
      "0\t1.00\tb\"itun i'm wait notanapolog :(\"\n",
      "0\t1.00\tb'layout match closest could find header :( someon help'\n",
      "0\t1.00\tb\"hayee :( hayee :( patwari mam ik' vision would say noth rather lil laugh\"\n",
      "0\t1.00\tb'awhhh ok ok :( see nalang class open hehe'\n",
      "0\t1.00\tb'last night one worst night :( pretti sure albanian women curs'\n",
      "0\t1.00\tb'air max tava pl pl pl pl :('\n",
      "0\t1.00\tb\"imagin would shatter dream :-( we'll let colleagu know sb\"\n",
      "0\t1.00\tb'want white french bulldog :('\n",
      "0\t1.00\tb\"aww poor wish help even though can't realli help much :(\"\n",
      "0\t1.00\tb\"i'm sorri see tweet :( 2 point x\"\n",
      "0\t1.00\tb'3 day without talk bae :('\n",
      "0\t1.00\tb\"pleas tweet someth i'm sad :( cheer pleas ..\"\n",
      "0\t1.00\tb'nakakapikon yung nagbabasa ka ng blog comment info full peopl ask damn question answer post :('\n",
      "0\t1.00\tb'lead caus cancer children five :('\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
      "0\t1.00\tb\"i'm sorri peopl shit respect person space :(\"\n",
      "0\t1.00\tb'thank send premium write instrument howev dastardli swine stole envelop :('\n",
      "0\t1.00\tb'im get sick tire pipol say im short ... short ... tini :('\n",
      "0\t1.00\tb'fair enough :( would anyon seem tad fuck  .. wiper wiper'\n",
      "0\t1.00\tb'supposedli one worst kernel devic :('\n",
      "0\t1.00\tb'today  job anoth fack intel extra mega care get bent socket pin :-( pcgame pcupgrad'\n",
      "0\t1.00\tb'1 pl download smosh movi free onlin hahahaha :('\n",
      "0\t1.00\tb'kik plawnew 837 kik kikmenow amateur sf snapm summer hotti :('\n",
      "0\t1.00\tb'omg way guy aswel :-( btw cat litter isnt good pregnanc make sure u dont go near'\n",
      "0\t1.00\tb'cannot view pictur unhappi seat bu :('\n",
      "0\t1.00\tb\"there' huge bag present luke can't open he' back work :-(\"\n",
      "0\t1.00\tb'last time went caerphilli find anywher sell local chees :-('\n",
      "0\t1.00\tb'eye realli bad today even glass :('\n",
      "0\t1.00\tb\"i'm omw  lol wish :(\"\n",
      "0\t1.00\tb'never see dad midnight bc work hard fuck :( growinguppoor'\n",
      "0\t1.00\tb'number one america    wish could buy money :('\n",
      "0\t1.00\tb'oh gosh say aw hun :( cuddl'\n",
      "0\t1.00\tb'apolog avail australia :('\n",
      "0\t1.00\tb'want finish death parad weekend :('\n",
      "0\t1.00\tb\"i'm frustrat plank time super inconsist idea i'm wrong :(\"\n",
      "0\t1.00\tb'rip tom moor ... love read comic book ... anoth great artist truli miss :('\n",
      "0\t1.00\tb'awww miss swear twitter day truli bore rather watch movi footbal :('\n",
      "0\t1.00\tb'sch get better ... :-('\n",
      "0\t1.00\tb'get bin osx chrome voiceov >:('\n",
      "0\t1.00\tb'  night :( devo xxx'\n",
      "0\t1.00\tb'hope wrong hulkhogan think know deep hurt wake kind unpleas :('\n",
      "0\t1.00\tb'grabe ang harsh huhuhu u like :('\n",
      "0\t1.00\tb\"sorri i'm fulli book :-( xx\"\n",
      "0\t1.00\tb'unfollow unni :('\n",
      "0\t1.00\tb\"panic' new album come 22nd :(\"\n",
      "0\t1.00\tb'bigbang need rest :( travel one place anoth must realli tire :('\n",
      "0\t1.00\tb'mention south africa mani time song time come south africa :('\n",
      "0\t1.00\tb\"hate can't rememb dream love share :(\"\n",
      "0\t1.00\tb'go america man r p victim louisiana :('\n",
      "0\t1.00\tb\"that' excus launch attack lizardz feel like shit deepli asham >:(\"\n",
      "0\t1.00\tb'sorri :( need get emerg engin home next 4 hour laura'\n",
      "0\t1.00\tb'never go sleep earli :( lol'\n",
      "0\t1.00\tb'home dormtel near st scho girl siya tho :( want help find onee'\n",
      "0\t1.00\tb'talk exclus :( u know carri u want play '\n",
      "0\t1.00\tb\"sorri nat school holiday .. :( i'm work 1pm\"\n",
      "0\t1.00\tb'everi time feta chees head start blaaaz sometim vision mess nausea come :('\n",
      "0\t1.00\tb\"oh that' good :( we'r awar issu abl top-up\"\n",
      "0\t1.00\tb'sharknado one hour never get back :('\n",
      "0\t1.00\tb'yeah leav read :-('\n",
      "0\t1.00\tb'cannot even sleep right :('\n",
      "0\t1.00\tb'look fun snapchat lilybutl 18 snapchat kikmenow amateur kikmeboy seduc hannib kiksext :('\n",
      "0\t1.00\tb'ed sheeran preform countri tonight ur go :('\n",
      "0\t1.00\tb'cannot seem get one powai :('\n",
      "0\t1.00\tb'feet cold point take sock okay stop :('\n",
      "0\t1.00\tb\"sinc model stop stock boot can't get can't even order onlin tell unsaf :(\"\n",
      "0\t1.00\tb\"summer holiday great i'm bore alreadi :(\"\n",
      "0\t1.00\tb'oh plan stream halo xbox one today :('\n",
      "0\t1.00\tb'drop dead fred use favorit movi wish :-('\n",
      "0\t1.00\tb\"rememb 2 gaon bad 6 infnt elig apink :( acub why'd u\"\n",
      "0\t1.00\tb'sad everi time apink event .. understand korean :(  '\n",
      "0\t1.00\tb':( know happen internet light connect drop thank kei'\n",
      "0\t1.00\tb'wish could hear food :('\n",
      "0\t1.00\tb'hanaaa birthday ya allah sorri wish van jn tak tau :( happi birthday gorgeou'\n",
      "0\t1.00\tb'happen soul si :('\n",
      "0\t1.00\tb'last present receiv  haha basta :('\n",
      "0\t1.00\tb'snapchat guy jasmingarrick snapchat kiksext chat sext addm mpoint hotmusicdeloco :('\n",
      "0\t1.00\tb'relat full lie :('\n",
      "0\t1.00\tb\"aw fuck there' dress bag floor dammit that' realli loud :(\"\n",
      "0\t1.00\tb'dhi  block twitter :( thank good time  play 8ball fakmarey'\n",
      "0\t1.00\tb\"would like abl work full time stuff move i'm consid take fulltim job :(\"\n",
      "0\t1.00\tb\"yeah i'v seen someth tinder fb root :( woman frustrat awkward\"\n",
      "0\t1.00\tb'food kitchen money wallet thank barcelona dad home month beet juic either :('\n",
      "0\t1.00\tb'dci today wish go :-('\n",
      "0\t1.00\tb'fair love war kapan updat :( oh ya udah dihapu hilang dari muka bumi want read someon give link '\n",
      "0\t1.00\tb'mani nasti narrow mind peopl :('\n",
      "0\t1.00\tb\"friend respect life :-( i'm sorri\"\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :('\n",
      "0\t1.00\tb\"can't watch phone sorri :( thank still <3\"\n",
      "0\t1.00\tb'taxi driver thought sophi work nintendo took us gate :( secur guard gave us dirtiest look haha'\n",
      "0\t1.00\tb'omg :-( love much monica see soon aaahhh'\n",
      "0\t1.00\tb'haha sound like  fun game :('\n",
      "0\t1.00\tb'school monday wanna meet new peopl :('\n",
      "0\t1.00\tb'... havent finish yet :('\n",
      "0\t1.00\tb'get hypixel :( want record grrr thestruggleisr geek gamer gamer youtub'\n",
      "0\t1.00\tb\"bummer :( apink' digit physic sale super great bc time album releas win\"\n",
      "0\t1.00\tb\"that' aw :( son miss physic exam chicken pox gave predict grade poor girl\"\n",
      "0\t1.00\tb'srsli tho bae look :('\n",
      "0\t1.00\tb\"ahm ... i'v never london ... holiday outsid ireland nearli 8 year :( never got chanc cash go\"\n",
      "0\t1.00\tb\"5am i'm wit ha damn headach :(\"\n",
      "0\t1.00\tb'dream met karli kloss sweet want take like bunch goofi photo :('\n",
      "0\t1.00\tb'realiz 3 watch concert video pcd extrem hard rn :('\n",
      "0\t1.00\tb'sorri antagonis call writer sever polit nudg ... :('\n",
      "0\t1.00\tb'delv enough seen heard fuck horrend :('\n",
      "0\t1.00\tb'duo drop doubl digit :( twice'\n",
      "0\t1.00\tb'theyr cute wish knew say :('\n",
      "0\t1.00\tb'grandad realli well :( worri much'\n",
      "0\t1.00\tb'lot peopl suggest still :('\n",
      "0\t1.00\tb'get pace maker oh :('\n",
      "0\t1.00\tb'molli deserv much higher place :('\n",
      "0\t1.00\tb'wanna good time one hit date much ask :('\n",
      "0\t1.00\tb'feel sucki w run nose cough sore throat fever :-('\n",
      "0\t1.00\tb'hate hot weather :( graduat ceremoni later boy gonna wear suit everyon go die'\n",
      "0\t1.00\tb\"let' go parti ... pant ew :( gross\"\n",
      "0\t1.00\tb'someon niall layout give :('\n",
      "0\t1.00\tb'that good enough reason :( pleas dont leav your one fav barb'\n",
      "0\t1.00\tb\"cooper i'm convinc neither 2 pleas i'm concern creasi win deputi :(\"\n",
      "0\t1.00\tb\"never remov minion car 23 day run batteri banana' :(\"\n",
      "0\t1.00\tb'want sponsor acc new even 1k :('\n",
      "0\t1.00\tb'told u guy bday today :( gift want paypal'\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :( saludo'\n",
      "0\t1.00\tb'hey someon text :('\n",
      "0\t1.00\tb'anoth dissapoint review :( bblogger makeup beauti'\n",
      "0\t1.00\tb\" <-- face everi guy pull get head feel bae' teeth dick :(\"\n",
      "0\t1.00\tb'wanna see paper town w :('\n",
      "0\t1.00\tb'get long last black pump :('\n",
      "0\t1.00\tb'omfg beauti place ever miss much :('\n",
      "0\t1.00\tb\"almost 12 i'm tire :(\"\n",
      "0\t1.00\tb\"honey brown sugar good ur lip :/ amount stuff use face could'v use food instead :(\"\n",
      "0\t1.00\tb'ikr hai would someon sell preciou thing :('\n",
      "0\t1.00\tb\"i'm tire :( got good sleep\"\n",
      "0\t1.00\tb'yea man ... suppos footbal tdi man plan fail :('\n",
      "0\t1.00\tb'pleas god wanna go work :('\n",
      "0\t1.00\tb\"pleas woe can't run six though live mexico :(\"\n",
      "0\t1.00\tb\"idk i'm get mini bu idk time i'm go get :(\"\n",
      "0\t1.00\tb\"hi harri sorri :( we'r work mast area caus intermitt servic work complet\"\n",
      "0\t1.00\tb'look fun snapchat janniecam snapchat kikm webcam snapchatm kikhorni musicbiz hotmusicdeloco :('\n",
      "0\t1.00\tb'braxton gone pro :( feel bad urban unpreced brainwash see tebow alex smith'\n",
      "0\t1.00\tb'wat okaaay huhu :( sayanggg find yah phone'\n",
      "0\t1.00\tb'1 taxi servic 2 second schedul blog post 3 housework bust :('\n",
      "0\t1.00\tb'mum say done leav step dad :-('\n",
      "0\t1.00\tb\"fanci kevin clifton strictli come danc much he' marri :( whyyi\"\n",
      "0\t1.00\tb\"awhh see guy would'v ram car :(\"\n",
      "0\t1.00\tb\"that' ok :( reason voucher can't use onlin\"\n",
      "0\t1.00\tb'kik smadvow 544 kik kikgirl lgbt photo model free hotti :('\n",
      "0\t1.00\tb\"everyone' work can't even get :(\"\n",
      "0\t1.00\tb'would love see acdc .. miss ticket :('\n",
      "0\t1.00\tb\"last stage today :( i'm go miss see almost everyday\"\n",
      "0\t1.00\tb'never give best thing take time weh :('\n",
      "0\t1.00\tb'guy add kik sprevelink 633 kik hornykik chat porno orgasm free webcamsex :('\n",
      "0\t1.00\tb'miss 11:11 :( btw wish follow  loveyoutilltheendcart '\n",
      "0\t1.00\tb'aw sound great better lunch :('\n",
      "0\t1.00\tb'want play sfv capcom plz :('\n",
      "0\t1.00\tb'tip onlin 6/7 winner yesterday 1 goal nice 20/1 :( hope similar strike rate today'\n",
      "0\t1.00\tb'u prob fun david'\n",
      "0\t1.00\tb'sooo burnt ... :-('\n",
      "0\t1.00\tb'need new phone :('\n",
      "0\t1.00\tb'could flat ab time ... @_ gym tayo g :('\n",
      "0\t1.00\tb'first two day kati summer back forth doctor suspect mening viral tonsil :( poorli girl '\n",
      "0\t1.00\tb'wanna go movi today one :('\n",
      "0\t1.00\tb\"i'v repli yet :( ang given work next 3 week  x\"\n",
      "0\t1.00\tb'dont either :( ugli cushion littl longer thank help'\n",
      "0\t1.00\tb'happi birthday pleas come back singapor :-(                 '\n",
      "0\t1.00\tb\"weigh risk premium go :( lot money 1 reason i'm keen get dog\"\n",
      "0\t1.00\tb\"can't see say ice cream uber app :( set locat\"\n",
      "0\t1.00\tb'wish could see ben :('\n",
      "0\t1.00\tb'think ever win petrofac cup :-( ;-)'\n",
      "0\t1.00\tb\"15 day ago danni took wig put onto mark' head want go back :( \"\n",
      "0\t1.00\tb'pathet emot burden.say like care hug respect toler call name :-('\n",
      "0\t1.00\tb\"man right wish someon would bring food offic i'm starv :(\"\n",
      "0\t1.00\tb'gosh cheaper malaysia :( worth 130 ship'\n",
      "0\t1.00\tb'weather set sleep respons :-('\n",
      "0\t1.00\tb'guy nooo :-( wow wow wow wow'\n",
      "0\t1.00\tb'boy snapchattimg happi bc friend ruin said wanna fun   :('\n",
      "0\t1.00\tb'tbh forget donat manag sin due hp corrupt restor data lost lor :('\n",
      "0\t1.00\tb'never dedic anyth :( smh lol'\n",
      "0\t1.00\tb'iren unni bite finger worriedli stare staff :( okay unni'\n",
      "0\t1.00\tb'guy add kik toneadi 46532 kik kikmeboy wife porno snapdirti premiostumundo sheskindahot :('\n",
      "0\t1.00\tb'hello info possibl interest jonatha he close beti :('\n",
      "0\t1.00\tb'la time right ye yunni im seriou :('\n",
      "0\t1.00\tb'develop releas good game school :('\n",
      "0\t1.00\tb'thank feel aw sick long time :('\n",
      "0\t1.00\tb'lot write boy section whdjwksja idk ziam write still fuck go 100k school :('\n",
      "0\t1.00\tb\"noo i'v ben watch like hawk week gut :(\"\n",
      "0\t1.00\tb'none hide behind shower curtain :('\n",
      "0\t1.00\tb':( want good least watchabl account neither'\n",
      "0\t1.00\tb'fav chees give migrin :('\n",
      "0\t1.00\tb\"still sad fix car window bc can't drive anywher without scare it'll shatter :(\"\n",
      "0\t1.00\tb\"oh :( do't think parcel come yodel hit follow dm track num chelsea\"\n",
      "0\t1.00\tb'wish could friend everybodi :( lmfaooo'\n",
      "0\t1.00\tb'oooouch poor pinki toe  good thing work podiatrist wait morn go hurri morn :('\n",
      "0\t1.00\tb\"miss old hous could hear parent door open act like i'm asleep :(\"\n",
      "0\t1.00\tb\"gusto ko ng rodic' :( someon share order one' big\"\n",
      "0\t1.00\tb'b-butt black cat bad luck ene'\n",
      "0\t1.00\tb'miss ticket show morn :( get tigermilk fix'\n",
      "0\t1.00\tb'ye hope next year :('\n",
      "0\t1.00\tb'use recommend east dulwich gone super crazi last five year :('\n",
      "0\t1.00\tb'last episod realli intens :( kagami n kuroko forev <3'\n",
      "0\t1.00\tb'want see :( sana makita na kita ang'\n",
      "0\t1.00\tb'buy book memem live finland :('\n",
      "0\t1.00\tb\"look amaz think saw meadowhal day actual unfortun weekend i'll work :(\"\n",
      "0\t1.00\tb\"rip lola :( i'll miss sorri di kita nadalaw last day thank everyth .. sa memori sa warm hug sa lahat labyu la\"\n",
      "0\t1.00\tb'dont want anyth happen bae :('\n",
      "0\t1.00\tb'even start trend wanna know jot truth :( zayniscomingbackonjuli 26'\n",
      "0\t1.00\tb'snapchat guy ivypowel 19 snapchat kikhorni chat girl like 4like travel phonesex :('\n",
      "0\t1.00\tb'fav emoticon right :( emoticon'\n",
      "0\t1.00\tb\"see i'd get eyebrow done prettylook wanna tri differ style :(\"\n",
      "0\t1.00\tb'aur ap bhi shamil ho :('\n",
      "0\t1.00\tb\"much past 2 day bc need save 3g feel like i'v miss tonn :(\"\n",
      "0\t1.00\tb'hey sorri hear :( check servic statu page  gen'\n",
      "0\t1.00\tb'hold back like way :('\n",
      "0\t1.00\tb\"amount i'm eat day ridicul know i'm suddenli alway hungri :(\"\n",
      "0\t1.00\tb'omg alli hug mani wrap arm around neck pull closer :-('\n",
      "0\t1.00\tb'want kitten badli oh god :('\n",
      "0\t1.00\tb'add snapchat jannygreen 22 snapchat kiksex xxx seduc tagsforlik nakamaforev hotel :('\n",
      "0\t1.00\tb'told suicid kill ran away forev :('\n",
      "0\t1.00\tb\"pleas follow conno i'm late time zone could freak never see :(\"\n",
      "0\t1.00\tb'biggest dream u follow brooo :('\n",
      "0\t1.00\tb\"i'm lone girl england :(\"\n",
      "0\t1.00\tb'time last week rout lovebox :( share day new prod'\n",
      "0\t1.00\tb'still happen lot unfortun still report peopl fall scam :('\n",
      "0\t1.00\tb'eric made still tell love :-( fake ass'\n",
      "0\t1.00\tb'need job learn drive fricken health put back :('\n",
      "0\t1.00\tb\"hi unfortun we'r unabl locat origin complaint :( pl dm us product detail we'll get back\"\n",
      "0\t1.00\tb'ehem   haha ala yeke :( okay fun kk jumpa next time     mayb '\n",
      "0\t1.00\tb'come home empti hous ape :('\n",
      "0\t1.00\tb\"i'v shit bestfriend :(\"\n",
      "0\t1.00\tb\"i'm gonna inact next month :(\"\n",
      "0\t1.00\tb\":( well tri someth keep mind bad thing we'r alway\"\n",
      "0\t1.00\tb\"i'v never seen felt snow :(\"\n",
      "0\t1.00\tb\"i'm work :( show get home 1.2 lost map accid lost build\"\n",
      "0\t1.00\tb'dont know huhu test kasi namin didnt finish ap test :-('\n",
      "0\t1.00\tb'reject section uk govt e-petit site die :('\n",
      "0\t1.00\tb'oh :( build suit'\n",
      "0\t1.00\tb\"wanna cut k i'm friend threw sleepov parti :( i'm pretend ok like usual\"\n",
      "0\t1.00\tb'love island finish feel like summer gone :('\n",
      "0\t1.00\tb'push client way host mayb better wait till next week :('\n",
      "0\t1.00\tb'someon say thank u goodby chri tomorrow pl :('\n",
      "0\t1.00\tb'could realli use hug mayb icecream :('\n",
      "0\t1.00\tb'lot good comic event seem kid :( part school program'\n",
      "0\t1.00\tb'look like dog food ... :( ugh'\n",
      "0\t1.00\tb'u guy togeth :('\n",
      "0\t1.00\tb'saw one doujin think killua get fuck ging bit sad caus didnt show dick hope ginggon :('\n",
      "0\t1.00\tb'comput broken wait fix miss much twitter guys.al love .. :('\n",
      "0\t1.00\tb\"joe i'm sick come round make soup :(\"\n",
      "0\t1.00\tb\"stori life :( haha look who' talk\"\n",
      "0\t1.00\tb\"taxi driver go realli fast i'm scare :(\"\n",
      "0\t1.00\tb'need small ladi walk back sore ... :('\n",
      "0\t1.00\tb'justin come scotland :('\n",
      "0\t1.00\tb\"ok i'll write u everi week :(\"\n",
      "0\t1.00\tb'curv tummi big .. get flat ab :('\n",
      "0\t1.00\tb\"feel sick stomach hurt i'v done cri night can't sleep :(\"\n",
      "0\t1.00\tb\"that' quot tweet :(\"\n",
      "0\t1.00\tb'listen album live franc :('\n",
      "0\t1.00\tb'time like style :| like :('\n",
      "0\t1.00\tb'account meant hit us lunch time :('\n",
      "0\t1.00\tb'pl stop say rude nicknam involv short im sick yall :('\n",
      "0\t1.00\tb'pl write english :( dont understand'\n",
      "0\t1.00\tb'snow cold gross ew big mess everyon scrape pile anyway :('\n",
      "0\t1.00\tb'hey went home na :('\n",
      "0\t1.00\tb'nigga dalla look like :( sale sare bandar varg'\n",
      "0\t1.00\tb'hammer right ... look set day ... :('\n",
      "0\t1.00\tb'feel like cri reason right :('\n",
      "0\t1.00\tb'ur crazi af hurt much bc u know shit aint real edit :('\n",
      "0\t1.00\tb\"although mum ask watch w  stilll :( read yet she'd ask question middl movi\"\n",
      "0\t1.00\tb'work 5 everyday exhaust :('\n",
      "0\t1.00\tb'tbh prefer apma 14 idk man there someth right year apma :('\n",
      "0\t1.00\tb'u give 350 ill give u 350 350 rt anyth pl :('\n",
      "0\t1.00\tb'lmao back get realli realli seriou ... :('\n",
      "0\t1.00\tb'wonder happen earlier realiz leadership :-( wakeupgop'\n",
      "0\t1.00\tb'wanna take adam candi eat :('\n",
      "0\t1.00\tb\"can't believ ill go work today wish :(\"\n",
      "0\t1.00\tb':( hate school u dont understand miss lot blame school'\n",
      "0\t1.00\tb\"like can't actual put pressur ankl hop around hous lost balanc fell :(\"\n",
      "0\t1.00\tb'last time one work :( fb gone check long time lol'\n",
      "0\t1.00\tb'asian ummm mayb ur film child pornographi slutsham fake suicid ect :-('\n",
      "0\t1.00\tb'poland faraway germani girl :( still believ one day meet'\n",
      "0\t1.00\tb\"good quit full time job last week can't find anoth one :(\"\n",
      "0\t1.00\tb':( well cooki better worth'\n",
      "0\t1.00\tb\"lmao catch point watch weekli i'll ok make 700 there' 800 ep :(\"\n",
      "0\t1.00\tb\"cake look amaz can't believ i'm miss :(\"\n",
      "0\t1.00\tb'keep forget area :-( want visit next time xx'\n",
      "0\t1.00\tb'make want watch whole film cgi kick oop pun start grumbl turn :('\n",
      "0\t1.00\tb'see delph anoth kit upset :('\n",
      "0\t1.00\tb\"miss :( haha x'   still rememb u hate xxx\"\n",
      "0\t1.00\tb\"we'r allow cheat meal :(\"\n",
      "0\t1.00\tb'fun osaka super junior :( would love watch comeback stage though'\n",
      "0\t1.00\tb'aytona hala guy get readi na mathird wheel :( jkjk'\n",
      "0\t1.00\tb'worst day ever :( pain ...'\n",
      "0\t1.00\tb'backtrack 3 hour :( ... time start actual util list'\n",
      "0\t1.00\tb'surviv anoth week without phone :('\n",
      "0\t1.00\tb'pat jay'\n",
      "0\t1.00\tb'icaru 500 awn unfortun fit 600 size :('\n",
      "0\t1.00\tb'6 day left :('\n",
      "0\t1.00\tb'give work old staff come polic later court look anoth job :( bye bye'\n",
      "0\t1.00\tb\"landlord like mp' though that' go easi :-(\"\n",
      "0\t1.00\tb'look old photo get sad :('\n",
      "0\t1.00\tb'oh :( hope everyth okay'\n",
      "0\t1.00\tb\"want want want follow want want want that' crazi :( pleas love (:  \"\n",
      "0\t1.00\tb'love warrior :-('\n",
      "0\t1.00\tb\"i'm watch gossip girl make sad :(\"\n",
      "0\t1.00\tb\"can't deal match unless ishii fuck everyth :(\"\n",
      "0\t1.00\tb'whatev stil l young >:-('\n",
      "0\t1.00\tb'look like cri look like cri bit :('\n",
      "0\t1.00\tb'come onlin follow peopl :/ z wrong u ignor sidharth :( :('\n",
      "0\t1.00\tb\"lol i'm lazi learn ps :(\"\n",
      "0\t1.00\tb\"sleep i'm\"\n",
      "0\t1.00\tb'oh ndabenhl :-( pleas let us know feel way sa'\n",
      "0\t1.00\tb\"well i'm p 41 8 page note thu far :-( i'v read twice\"\n",
      "0\t1.00\tb\"ye unfortun done whole troubleshoot box i'm still issu :(\"\n",
      "0\t1.00\tb'dem free tix big bang concert :('\n",
      "0\t1.00\tb'your gonna guess what insid box :('\n",
      "0\t1.00\tb'idk :-( mayb think boob fun play'\n",
      "0\t1.00\tb\"we'r sorri :( longer abl access site automat redirect local site\"\n",
      "0\t1.00\tb'mad gigi dream hang didnt want talk :('\n",
      "0\t1.00\tb'carter deserv hate :( deserv singl corn chip deserv two corn chip'\n",
      "0\t1.00\tb'still 6 hour work left omg :('\n",
      "0\t1.00\tb\"freak humid think never get use even i'm 100 japanes :(\"\n",
      "0\t1.00\tb\":( longer admir consist jason shackell' hair podcast\"\n",
      "0\t1.00\tb\"envi peopl get meet pictur laura taylor lana etc they'r lucki meet babi :(\"\n",
      "0\t1.00\tb':-( pleas notic gonna spam u till u follow babi'\n",
      "0\t1.00\tb'kik twer 782 kik kikhorni lesbian hornykik girl countrymus hornykik :('\n",
      "0\t1.00\tb'word explain much miss hope know :('\n",
      "0\t1.00\tb'ok :-( w hahaahahahaha'\n",
      "0\t1.00\tb'sm1 bad mean got mutil robot shot wall know could destroy ... >:('\n",
      "0\t1.00\tb'good morn twitter friend hope amaz day freakin tire need sleep :('\n",
      "0\t1.00\tb'good alway  fli back today though :('\n",
      "0\t1.00\tb'wish twitter would support audio record would send yall snippet amaz :('\n",
      "0\t1.00\tb\"master figur turn led gold we'r doom :(\"\n",
      "0\t1.00\tb\"want pull nighter i'm sleepi alreadi :(\"\n",
      "0\t1.00\tb'ever wanna sleeep :( let mee'\n",
      "0\t1.00\tb\"oh allow vote teen choic award :( said i'm area\"\n",
      "0\t1.00\tb'want icon selfi jack :('\n",
      "0\t1.00\tb\"twitter turn much complic can't find trend :(\"\n",
      "0\t1.00\tb\"right i'm tire :( good night alli love  \"\n",
      "0\t1.00\tb' wish channel watch :( usual watch onlin laptop anymor :/'\n",
      "0\t1.00\tb\"would want peopl understand busi daili schedul  c'mon guy bare even get 5 hour sleep :-(\"\n",
      "0\t1.00\tb'want bday answer pleas  idk leh :( b'\n",
      "0\t1.00\tb'yeahhh ... :( tht corner b rememb alway coz gg ... :( n bb didnot show th clip episod :('\n",
      "0\t1.00\tb'releas onlin album know :('\n",
      "0\t1.00\tb'   thank san  wanna chang avi usanel :('\n",
      "0\t1.00\tb'noo sad time :( rubi replac shuffl act'\n",
      "0\t1.00\tb'got excit micha rt fave tweet creep :-('\n",
      "0\t1.00\tb\"anyon wanna friend i'm lone girl england bore hell throughout summer holiday .. :(\"\n",
      "0\t1.00\tb'whose idea ave tripl busi lectur friday afternoon 2-5 :('\n",
      "0\t1.00\tb\"i'm allow give blood :(\"\n",
      "0\t1.00\tb'six month ago bottom jaw second tooth back broke quarter ... thing chew gum :('\n",
      "0\t1.00\tb'go sleep 12 last night rn im :('\n",
      "0\t1.00\tb'twitter decid take featur away :-('\n",
      "0\t1.00\tb\"i'm okay tire plan go today rain :(\"\n",
      "0\t1.00\tb'tzelumxoxo dont realli use line ipod :-( use whatsapp actual'\n",
      "0\t1.00\tb'buy someth drank caus noth :('\n",
      "0\t1.00\tb\"slept day can't sleep :(\"\n",
      "0\t1.00\tb'peopl :( r u'\n",
      "0\t1.00\tb'kik ouliv 70748 kik kikmenow photo babe loveofmylif brasileirao viernesderolenahot :('\n",
      "0\t1.00\tb'longmorn 30 suppos replac tobermori 32 ... begin strong tail miss :('\n",
      "0\t1.00\tb'wait love recuerda tanto bath :('\n",
      "0\t1.00\tb'shit .. suggest na suck muna :( know hard nurs import right'\n",
      "0\t1.00\tb'much await excit go bore friday weekend :('\n",
      "0\t1.00\tb'hp curs child book play :('\n",
      "0\t1.00\tb'ur crazi af u want hurt urslef edit stuff lime u know shit aint real :( '\n",
      "0\t1.00\tb'omg r hahah :( omg debut mo na next year :('\n",
      "0\t1.00\tb'wish could meet boy one day :('\n",
      "0\t1.00\tb'tri keep hous use buy truckload miss 13 favour :('\n",
      "0\t1.00\tb\"best spectat sail uk w'end bbc show highlight :(  fli  foil ac45 catamaran\"\n",
      "0\t1.00\tb'guy add kik peli 829 kik kikmenow fuck model amateur elfindelmundo sextaatequemfimseguesdvcomvalentino :('\n",
      "0\t1.00\tb\"could pl take care server befor send valu long can't get :(\"\n",
      "0\t1.00\tb'love babi sweet cinnamon best :('\n",
      "0\t1.00\tb'mtap tomorrow mean sleep earli tonight :-('\n",
      "0\t1.00\tb'peng bestfriend :( psygustokita'\n",
      "0\t1.00\tb'wish could dm cute thing :( prefer cute kitten puppi'\n",
      "0\t1.00\tb'still get paper engg go back cmc mage okay :('\n",
      "0\t1.00\tb\"that' kind dumb statement like moodsw like make tweet say hate termin diseas :( like wow uniqu\"\n",
      "0\t1.00\tb'slow news day :-('\n",
      "0\t1.00\tb':-( pleas notic men'\n",
      "0\t1.00\tb'even though seen whole peep show multipl time noth could prepar netflix remov seri 1 7 ... :('\n",
      "0\t1.00\tb\"hi oh :( let' take look pleas chat us\"\n",
      "0\t1.00\tb'sad moment u r leav two day :('\n",
      "0\t1.00\tb\"lol 2am last night face though peopl think i'm perpetu piss sad b c rest bitchfac :(\"\n",
      "0\t1.00\tb'peopl wish birthday :('\n",
      "0\t1.00\tb'let feel like complet shit :('\n",
      "0\t1.00\tb\"uh huh :( two time i'm super clever like\"\n",
      "0\t1.00\tb'fave unfollow :(  '\n",
      "0\t1.00\tb'realli want visit iceland :('\n",
      "0\t1.00\tb'zayn_come_back_we_miss_y <3 <3 :( much'\n",
      "0\t1.00\tb'internet total bitch'\n",
      "0\t1.00\tb'wish could go shop whole weekend taken :( annoy'\n",
      "0\t1.00\tb'peopl usual stay asleep wtf :('\n",
      "0\t1.00\tb'payback like video tri help get kit kat :('\n",
      "0\t1.00\tb'one come smoke :('\n",
      "0\t1.00\tb'happi birthday be enjoy day beb love miss sooo muchi :( see soon sana     '\n",
      "0\t1.00\tb\"alway fall asleep earli i'm text someon :(\"\n",
      "0\t1.00\tb'say icecream avail :('\n",
      "0\t1.00\tb\"ask go bathroom :-( i'm realli enjoy show though\"\n",
      "0\t1.00\tb'need clear head :('\n",
      "0\t1.00\tb\"babi went back sleep i'm bore :-(\"\n",
      "0\t1.00\tb\"naw :( deep dream novel hope there'd invis page beast scuttl around there' worm track\"\n",
      "0\t1.00\tb\"bad guy cant attend today' music bank :(\"\n",
      "0\t1.00\tb'jessica call quit power ab 5:15 :-('\n",
      "0\t1.00\tb'lost us friend ... :( one start argument'\n",
      "0\t1.00\tb'hope fun vidcon rlli bum couldnt go :( love sunshin   '\n",
      "0\t1.00\tb\"yepp .. i'm get bore :(\"\n",
      "0\t1.00\tb'last night good :(    '\n",
      "0\t1.00\tb'want biscuit slather chocol definit tax boo :('\n",
      "0\t1.00\tb'investig answer especi pup die within free insur period :('\n",
      "0\t1.00\tb'happi princess today :( sigh get work done x purpl princess edit'\n",
      "0\t1.00\tb'word explain way miss :('\n",
      "0\t1.00\tb'think need 2 year train beat record sia :('\n",
      "0\t1.00\tb'niram ya geng :( fikri anna 6 other tirtagangga hotel '\n",
      "0\t1.00\tb'char im realli sick :( one font size 8 minion float font size 12 sick kid pl :('\n",
      "0\t1.00\tb'bestfriend like ed plsss :('\n",
      "0\t1.00\tb'riprishikeshwari soul rest peac :('\n",
      "0\t1.00\tb':( unfollow like 300 peopl caus follow 1k peopl difficult'\n",
      "0\t1.00\tb'enough time listen artist music :-('\n",
      "0\t1.00\tb'alway go terribl time :('\n",
      "0\t1.00\tb':-( challeng though pleas check fb page entri rather substitut thank'\n",
      "0\t1.00\tb'that life get call peopl havent seen 20 year alway favour'\n",
      "0\t1.00\tb'omg :( everi time would complain skin mom everyon get pimpl like although'\n",
      "0\t1.00\tb'letsfootbal atk greymind 43 break news chri gayl say cricket 2-3 month due back surgeri :( cplt 20 cp '\n",
      "0\t1.00\tb\"miss hannah montana that' raven sooo much :-(\"\n",
      "0\t1.00\tb'gonna okay love :('\n",
      "0\t1.00\tb'awh :-(  stay '\n",
      "0\t1.00\tb'last full night greec :( opu inner pleasur'\n",
      "0\t1.00\tb'disappoint hear especi invest one :-('\n",
      "0\t1.00\tb'three 190 row 33 :('\n",
      "0\t1.00\tb'im watch disney channel :('\n",
      "0\t1.00\tb'nobodi pick one right :('\n",
      "0\t1.00\tb'someon gift :( calibraskaep need bad'\n",
      "0\t1.00\tb'lol got split hard lol singl singl split collat spilt singl singl spilt singl split singl singl collat :('\n",
      "0\t1.00\tb'hope feel better jay :('\n",
      "0\t1.00\tb'ye mani time :( quitkarwaoyaaro'\n",
      "0\t1.00\tb'disappoint :( never ever lucki handl jabongatpumaurbanstamped'\n",
      "0\t1.00\tb'hello need 2.5 g2a code pl :('\n",
      "0\t1.00\tb'thesi thesi thesi thesi thesi thesi thesi thesi thesi thesi sherep nemen ng behey ke :('\n",
      "0\t1.00\tb'realli want anoth tattoo :('\n",
      "0\t1.00\tb'someon get mad reec vm tomorrow pleas never one :('\n",
      "0\t1.00\tb\"let' go  cast vote got 7 today :(\"\n",
      "0\t1.00\tb'omg :( gonna happen yeah wish ..'\n",
      "0\t1.00\tb'found flight home 13 hour long :('\n",
      "0\t1.00\tb'ugh yuck :( make sure rest next two day'\n",
      "0\t1.00\tb'omg sad birthday sorri didnt send someth ystrday ill later cuz dont phone :('\n",
      "0\t1.00\tb'seolhyun isnt first ep she film drama :('\n",
      "0\t1.00\tb'alway get like 5 4 piec spici nugget :('\n",
      "0\t1.00\tb'fellow morn owl sure notic sun alreadi rise bit later ... sign winter come :('\n",
      "0\t1.00\tb'rain whole day mumbai  feel like work :('\n",
      "0\t1.00\tb\"best get littl ladi pj' need go wallpap mom :( realli cba today\"\n",
      "0\t1.00\tb'ye pl far need :('\n",
      "0\t1.00\tb'even know could get hotter fml :('\n",
      "0\t1.00\tb\"aw bless made smile i'm miss :(\"\n",
      "0\t1.00\tb'hi life :('\n",
      "0\t1.00\tb'mood whole day :-('\n",
      "0\t1.00\tb'almost done park rec :( lol'\n",
      "0\t1.00\tb'u come back :('\n",
      "0\t1.00\tb\"hi we'r sorri hear :( log abl see inform need\"\n",
      "0\t1.00\tb'honest miss dubai dubai miss want come back :('\n",
      "0\t1.00\tb'final get star driver srw mobag :('\n",
      "0\t1.00\tb'sad reason suck u dunno stop sad u gotta chill ur room listen music b alon :('\n",
      "0\t1.00\tb'stream pass melon pleas stream much  digit bad :('\n",
      "0\t1.00\tb'snapchat jannygreen 18 snapchat kiksex hot teen lgbt goodmus sexcam :('\n",
      "0\t1.00\tb'first saw gnr open alic cooper famou parti backstag alic r slash :('\n",
      "0\t1.00\tb\"tag  i'm sorri lot music prolli know half :( also\"\n",
      "0\t1.00\tb'love u fuck know :('\n",
      "0\t1.00\tb\"aww i'v alreadi left would come said hey seen sooner :( good night x\"\n",
      "0\t1.00\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n",
      "0\t1.00\tb'tri make prison style alcohol fail badli end massiv dose shit ... :('\n",
      "0\t1.00\tb'fair still look cute :('\n",
      "0\t1.00\tb\"m' voic :( lord help\"\n",
      "0\t1.00\tb'want bandana bottl set huhuh :('\n",
      "0\t1.00\tb'meet steven kid listen :('\n",
      "0\t1.00\tb\"i'm fuck jealou everyon holiday :(\"\n",
      "0\t1.00\tb'terribl thing doin yoga work stop start ... awalmu njareeem :('\n",
      "0\t1.00\tb'nooo last day today :('\n",
      "0\t1.00\tb'care maggi psycho bitch want kill take two day vacat twitter :( wahhh'\n",
      "0\t1.00\tb'realli look forward work tonight though :('\n",
      "0\t1.00\tb'go home blue :-( back monday hiby social action plan shareyoursumm'\n",
      "0\t1.00\tb'pro soccer play would cool :('\n",
      "0\t1.00\tb\"hey girl must.b dairi produxt want can't lactos intoler :(\"\n",
      "0\t1.00\tb'midland ye thank depress weather forecast word rain mention sever time :-('\n",
      "0\t1.00\tb'old one day ... :('\n",
      "0\t1.00\tb\"liter spent day yesterday sleep bed i'm still absolut fuck knacker today :(\"\n",
      "0\t1.00\tb'new sandra bland footag realli ice cake .. heartbreak drag lifeless bodi polic car :('\n",
      "0\t1.00\tb\"we'r sorri feel way shell :( regularli review price offer best\"\n",
      "0\t1.00\tb'sorri hear jess :( quickest way call 44 7782 333 333 add account within 72 hour'\n",
      "0\t1.00\tb'girlll u hear abt possibl tsunami us west indi im island im afraid lol :('\n",
      "0\t1.00\tb'would love see w nick pick one waist tv time never work anyway :('\n",
      "0\t1.00\tb'hulk hogan news racial tirad :( thought couldnt stoop lower role thunder paradis'\n",
      "0\t1.00\tb'sorri forget import day huhu fight im sorri :('\n",
      "0\t1.00\tb\"break habit lifetim ... i'm play call duti game .. :(\"\n",
      "0\t1.00\tb'apolog annoy much snapchat :('\n",
      "0\t1.00\tb'adult take thing seriou realli miss kid :('\n",
      "0\t1.00\tb\"someon come i'll make popcorn watch movi pl cuddl :(\"\n",
      "0\t1.00\tb'final home drive sinc 5am :('\n",
      "0\t1.00\tb'bade fursat se usey banaya hai usey uppar waal ney afso hums dur hai wo :('\n",
      "0\t1.00\tb\"oh realli :( saw gif post seen realli happi mayb peopl who'd happi naruhina\"\n",
      "0\t1.00\tb'that english namee :( haiqal isnt english name rememb'\n",
      "0\t1.00\tb\"that' 360hr summer end ... huhuhu miss picc lot :(\"\n",
      "0\t1.00\tb'instor card expir june havent sent new one :('\n",
      "0\t1.00\tb'mcountdown pre-vot begin 5th place :('\n",
      "0\t1.00\tb'felt bad iren forgot go say :('\n",
      "0\t1.00\tb\"wow that' great what' usernam :(\"\n",
      "0\t1.00\tb'minho still injur :( get well oppa miss <3'\n",
      "0\t1.00\tb\"can't even deliv ice cream tri twice ice cream van busi :(\"\n",
      "0\t1.00\tb'guy add snapchat amargolonnard snapchat kikmeboy sexi french dirtykik newmus sexcam :('\n",
      "0\t1.00\tb'u chang u say chang tsk :('\n",
      "0\t1.00\tb'damn .. wish could sleep :('\n",
      "0\t1.00\tb'like join marin :-('\n",
      "0\t1.00\tb\"i'm sad yall thought real :(\"\n",
      "0\t1.00\tb\"i'm go :( kailan ba may tix ka\"\n",
      "0\t1.00\tb'hotel dog pretti sad brother sister got separ :('\n",
      "0\t1.00\tb\"lmao :( natur hair i'v seen ugli swear\"\n",
      "0\t1.00\tb'nope :( best place get first pre-ord amazon'\n",
      "0\t1.00\tb'sunday littl abl get much year busi :( look great tho'\n",
      "0\t1.00\tb'go away ... must go dentist :( poorkid'\n",
      "0\t1.00\tb'thank lack time problem :('\n",
      "0\t1.00\tb'ur cat super nice cuddli suddenli scratch tri bite :( like trust anymor'\n",
      "0\t1.00\tb'hit thumb :('\n",
      "0\t1.00\tb'almost got see cute ankl sock :('\n",
      "0\t1.00\tb'honestli point care compens want abl play bf :('\n",
      "0\t1.00\tb'sleep last night n help wake 8 morn n phone even charg :('\n",
      "0\t1.00\tb'pleas tell everyon didnt go vidcon :('\n",
      "0\t1.00\tb'need like tell na na okay lang :('\n",
      "0\t1.00\tb'heart current tortur :( lol wat'\n",
      "0\t1.00\tb'femal student kill cast love kirkiri :('\n",
      "0\t1.00\tb\"i'v run bread feel well enough make :(\"\n",
      "0\t1.00\tb\":( broken heart that' phase worri\"\n",
      "0\t1.00\tb'stop sour :( suit  '\n",
      "0\t1.00\tb'wonho remind someon >:( cant put finger'\n",
      "0\t1.00\tb'wont work today feel ill :( tummi bug xx'\n",
      "0\t1.00\tb'visual studio 2015 got switch alreadi broke lot code :('\n",
      "0\t1.00\tb\"serious unfair ask question readi answer win :( ='( busi ask question\"\n",
      "0\t1.00\tb'pap spirit anim  dont :('\n",
      "0\t1.00\tb\"total ship danzel i'm craft can't go vidcon would sell soul devil get hug dan phil :(\"\n",
      "0\t1.00\tb\"he' gona miss sheff utd york probabl wait visa leav gim 1 friendli bet start season bench :(\"\n",
      "0\t1.00\tb'lead us gone :('\n",
      "0\t1.00\tb'point yolo sort die lactos ... ugh like 3 month pregnant bloat :('\n",
      "0\t1.00\tb'realli sorri cant find imag mention :('\n",
      "0\t1.00\tb'im get olli tweet notif :('\n",
      "0\t1.00\tb'go home tomorrow :-('\n",
      "0\t1.00\tb':( world alterni kill realli sad leav place everyon die'\n",
      "0\t1.00\tb'everyon gonna talk abt rat boy today bc :('\n",
      "0\t1.00\tb'earth assum rain london like influenc overal warm dri past week continent europ :('\n",
      "0\t1.00\tb'rememb fab four 24 hour call damn miss much :('\n",
      "0\t1.00\tb'  r u sure u want b r u h bare surviv w tank top :('\n",
      "0\t1.00\tb'konami polici pe pc version happen version :('\n",
      "0\t1.00\tb'ummm ok new develop :('\n",
      "0\t1.00\tb\"guy never :(  aw sorri we'r realli bust atm .. shall back soon\"\n",
      "0\t1.00\tb'bad :( like video mani pervers'\n",
      "0\t1.00\tb'first time go school without bracelet :( feel odd'\n",
      "0\t1.00\tb'im twin follow back bylfnnz :('\n",
      "0\t1.00\tb'unfair .. u ban film pakistan :('\n",
      "0\t1.00\tb\"hmmm 10 min get train i'm current 15 min away :( failsatlif\"\n",
      "0\t1.00\tb'complet agre press :('\n",
      "0\t1.00\tb'im super duper tire :('\n",
      "0\t1.00\tb'bore time :( know ...'\n",
      "0\t1.00\tb'hull support expect misser week :-('\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Predict with your own tweet\n",
    "\n",
    "In this part you can predict the sentiment of your own tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.427688872164715\n"
     ]
    }
   ],
   "source": [
    "# Test with your own tweet - feel free to modify `my_tweet`\n",
    "my_tweet = 'I am happy because I am learning :)'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this assignment. See you next week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
